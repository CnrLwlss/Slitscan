<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print" />
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>Slitscan by CnrLwlss</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>Slitscan</h1>
          <h2>Slitscan from stack of images with Python</h2>
        </header>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/CnrLwlss/Slitscan/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/CnrLwlss/Slitscan/tarball/master" id="download-tar-gz" class="button"><span>Download .tar.gz</span></a>
          <a href="https://github.com/CnrLwlss/Slitscan" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

        <hr>

        <section id="main_content">
		<h2>Example Output</h2>
		  <p>These images and videos were generated by running the Python script using frames from a video of me rotating very slowly on a chair as input.</p>
		  <h3>Still Images</h3>
			<p>
			<a href="http://www.flickr.com/photos/conchur/8012158402/" title="Horiz00570 by Conor Lawless, on Flickr"><img src="http://farm9.staticflickr.com/8178/8012158402_086e5a57aa.jpg" width="500" height="181" alt="Horiz00570"></a>
			  
			<a href="http://www.flickr.com/photos/conchur/8012150999/" title="Horiz00655 by Conor Lawless, on Flickr"><img src="http://farm9.staticflickr.com/8305/8012150999_6ccf2471f8.jpg" width="500" height="181" alt="Horiz00655"></a>
			</p>	  
		  
		  <h3>Videos</h3>
			<p>
			<iframe src="http://player.vimeo.com/video/49968391?title=0&amp;byline=0&amp;portrait=0&amp;autoplay=1&amp;loop=1" width="300" height="169" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>

			<iframe src="http://player.vimeo.com/video/49967683?title=0&amp;byline=0&amp;portrait=0&amp;autoplay=1&amp;loop=1" width="300" height="113" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>

			<iframe src="http://player.vimeo.com/video/49966521?title=0&amp;byline=0&amp;portrait=0&amp;autoplay=1&amp;loop=1" width="300" height="533" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>

			<iframe src="http://player.vimeo.com/video/49966520?title=0&amp;byline=0&amp;portrait=0&amp;autoplay=1&amp;loop=1" width="300" height="533" frameborder="0" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>
			</p>
		
		  <h2>Introduction</h2>
          <p> <a href="">Slitscanning</a> is a technique for constructing synthetic still images from a moving subject.  Analogue slit-scanning has been carried out since the <a href="http://www.flong.com/texts/lists/slit_scan/#dtb">early days</a> of photography by sequentially exposing thin slices of film over time to create a single image.  A digital alternative can be constructed using a video camera, replacing complicated moving apertures by digital extraction of strips (typically one pixel wide) from video frames.  Although slitscanning can be used to represent motion in one frame without motional blur, the synthetic images generated are often difficult (though not impossible) to interpret visually and look positively weird.  Trying to interpret the resulting images is the main appeal of this technique.</p>
		  
		  <p>Recently, while attending molecular biology conferences, I have noticed many presentations which include so-called <a href="http://fiji.sc/wiki/index.php/Generate_and_exploit_Kymographs">kymographs</a>, which are visualisations designed to summarise observations of dynamics captured by video or time-lapse microscopy (usually observing fluorescently labelled proteins).  Kymographs are constructed by synthesising a single image from strips extracted from digital video frames, and as far as I can tell, their main function is to provide a means to represent some of the three-dimensional data contained in a video (colour or intensity in x and y spatial dimensions and how that varies in the time dimension z) in a two-dimensional image appropriate for publication in a journal article. This is precisely a digital implementation of slitscanning.</p>
		  
		  <p>This piece of <a href="http://www.python.org/">Python</a> code builds slitscans by taking slices through stacks of digital images.  It works by loading the pixels of each frame of a video into a stack (a multi-dimensonal array) and constructing images by taking slices through the stack.  Straight horizontal or vertical slices through the stack give regular slitscans (equivalent to kymographs), but any other diagonal slicing through the stack is also possible.  The connection between the dynamics captured in the video and the resulting image are slightly different in each case.</p>
		  
		  <h2>Hardware Requirements</h2>
		  <p>To create good quality slitscans, the source video should really be of high resolution (i.e. at least <a href="http://en.wikipedia.org/wiki/720p">720p</a> HDV).  Extracting frames from BluRay video for example would give <a href="http://en.wikipedia.org/wiki/1080p">1080p</a>.  To create a relatively "square" stack of images, we will need at least as many frames as the largest dimension of a single video frame.  For example, for 1080p video, we will need at least 1920 frames, which is 64 seconds worth of footage at 30 FPS.  We will need to store these frames in RAM, uncompressed to be able to perform quick slicing.  For HD projects this requires quite a lot of RAM (~12 Gb) and 64-bit computing.</p>  
		  
		  <h2>Software Requirements</h2>
		  <p>This script obviously requires <a href="http://www.python.org/">Python</a>, but it also requires the <a href="">Python Imaging Library</a> (PIL) for opening and saving frames and images, as well as the <a href="">NumPy</a> package which extends Python to include multi-dimensional arrays and a suite of fast functions for operating on them.  Currently I split the source video into individual .png frames outside of Python, storing them temporarily on my hard-drive.  If I could have incorporated reading in a video file and serving up frames one at a time into the script, it would improve speed and greatly reduce the requirement for hard-drive space.  The <a href="http://code.google.com/p/pyffmpeg/">pyFFMPEG</a> project looked promising for extracting frames from video files at first, however it doesn't seem to be compatible with the latest version of <a href="http://ffmpeg.mplayerhq.hu/">FFMPEG</a> and it is not currently being developed, so I am reluctant to use it.  I recommend that users split video into individual .png images, using <a href="http://ffmpeg.mplayerhq.hu/">FFMPEG</a> directly, or under Windows, using the amazing <a href="http://www.virtualdub.org/">VirtualDub</a>.</p>
		  
		  <p>To access the large amounts of RAM we will require, we need a 64-bit operating system as well as 64-bit builds of Python and the required Python packages.  This is relatively straightforward under Linux, if you have a 64-bit machine, the default packages for installation (in Ubuntu for example) are all compiled for 64-bit computing.  Under Windows, you need to be a little more careful.  Choose the 64-bit version from the Python <a href="http://www.python.org/download/">download</a> page.  Sadly, the PIL and NumPy developers are not so Windows friendly, however you can download 64-bit binaries for these (and many other) science-related packages, generated by Christophe Gohlke <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/">here</a>.</p>

		  <h2>Running Script</h2>
		  <p>Make sure that all of your individual .png frames are numbered sequentially, and that the numbers are sensibly padded (e.g. Frame000001.png instead of Frame1.png).  Place all frames into a single directory and edit the <code>inroot</code> string to point to a frame in that directory.  Similarly, choose an output directory and replace the <code>outdir</code> string with the path to that directory.  The script will find all available frames, calculate the amount of RAM currently available on your machine, load as many frames as it can into memory and then generate a series of slices through the image stack.  If a series of slices can be generated, they will be and these series can be combined (again using FFMPEG or VirtualDub) to create a new video for example.</p>
		  
        </section>

        <footer>
          Slitscan is maintained by <a href="https://github.com/CnrLwlss">CnrLwlss</a><br>
          This page was generated by <a href="http://pages.github.com">GitHub Pages</a>. Tactile theme by <a href="http://twitter.com/jasonlong">Jason Long</a>.
        </footer>

        
      </div>
    </div>
  </body>
</html>